{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bef0acd-4ad8-43b7-b638-e2d89c77e72b",
   "metadata": {},
   "source": [
    "# Workflow and Automation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2c3d3f-434c-4f1e-b101-bc0857b2c2bd",
   "metadata": {},
   "source": [
    "This notebook is dedicated to setting various functions that will assist us in EDA, modeling, performance tuning, and interpretation of results for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "543141cc-3b9d-4ec2-9752-0cdfb9a7c805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "\n",
    "## metrics\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, recall_score, precision_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a290c5-cb76-4623-bd08-5ed414f32be4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Find NA values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae0f6e1-c2de-4e84-869f-492cd0e16c68",
   "metadata": {},
   "source": [
    "This function will find all NA values, and capture the sum of NAs in their respective column, in descending order, **only** if there are NAs present.\n",
    "\n",
    "**Input:** Dataframe <br />\n",
    "**Output:** Series of NA counts for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d310a887-9e65-442b-b1b2-82add485c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def na_only(df):\n",
    "    na_ser = df.isna().sum().sort_values(ascending=False)[lambda x: x > 0]\n",
    "    if na_ser.empty:\n",
    "        return 0\n",
    "    else:\n",
    "        return na_ser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb2d30c-538a-4d80-b203-23d0be23dffd",
   "metadata": {},
   "source": [
    "### Find Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29fa94ab-4358-4f8c-9dcd-03cbe8732d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_corr(df, threshold):\n",
    "    return df.corr().style.applymap(lambda x: 'color: red' if ((x > threshold or x < (threshold * -1)) and x != 1) else 'color: black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df86c0cb-2b8e-40c8-a1d8-aef67e4bf349",
   "metadata": {},
   "source": [
    "### Find Positive Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58efa335-348b-4a30-b912-390ae87c7a4f",
   "metadata": {},
   "source": [
    "This function will return the positively correlated variables relative to the response variable of choice, in descending order.\n",
    "\n",
    "**Input:** Dataframe, String name of response variable <br />\n",
    "**Output:** Series of positively correlated features relative to the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "301fc44b-da2b-4c56-a358-7c715674f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_cor(df, response):\n",
    "    all_cor = df.corr()[response].sort_values(ascending=False)[1:] #exclude the response itself from the series\n",
    "    return pd.Series({k:v for k, v in all_cor.items() if v > 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa538a4-2e7f-40e2-a6eb-989dd59d5ae0",
   "metadata": {},
   "source": [
    "### Find Negative Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af45b698-5636-40cb-b191-200eb5c27ed4",
   "metadata": {},
   "source": [
    "This function will return the negatively correlated variables relative to the response variable of choice, in ascending order.\n",
    "\n",
    "**Input:** Dataframe, String name of response variable <br />\n",
    "**Output:** Series of negatively correlated features relative to the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b339a30-0e24-493c-bcd1-2def73c51fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_cor(df, response):\n",
    "    all_cor = df.corr()[response].sort_values(ascending=False)[1:] #exclude the response itself from the series\n",
    "    return pd.Series({k:v for k, v in all_cor.items() if v < 0}).sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d1f128-eaa7-481d-9c30-5b75f66c180e",
   "metadata": {},
   "source": [
    "### Model Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae319cd9-cfe0-47f9-a2eb-808e9b500a5a",
   "metadata": {},
   "source": [
    "This function prints an aggregate of model performance metrics:\n",
    "- Coefficient of Determination ($R^2$)\n",
    "- Mean Absolute Error ($MAE$)\n",
    "- Root Mean Squared Error ($RMSE$)\n",
    "- Mean Squared Error ($MSE$)\n",
    "\n",
    "This function can be used for both testing and training metrics. Ensure that the response and prediction data are of the same dimension.\n",
    "\n",
    "**Input:** Response data, prediction data, boolean that indicates training or testing data <br />\n",
    "**Output:** Printout of all metrics, distinguished as training or test (aka \"unseen\") data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "961aa15c-b84a-4bd3-85b3-67861d4abe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y, preds, is_train=False):\n",
    "    r2 = r2_score(y, preds)\n",
    "    mse = mean_squared_error(y, preds)\n",
    "    rmse = mean_squared_error(y, preds, squared=False)\n",
    "    mae = mean_absolute_error(y, preds)\n",
    "    \n",
    "    if is_train:\n",
    "        print(f'''\n",
    "        Training Data Performance\n",
    "        ------\n",
    "        Coefficient of Determination: {r2}\n",
    "        Mean Absolute Error: {mae}\n",
    "        Root Mean Squared Error: {rmse}\n",
    "        Mean Squared Error: {mse}\n",
    "        ''')\n",
    "    else:\n",
    "        print(f'''\n",
    "        Unseen Data Performance\n",
    "        ------\n",
    "        Coefficient of Determination: {r2}\n",
    "        Mean Absolute Error: {mae}\n",
    "        Root Mean Squared Error: {rmse}\n",
    "        Mean Squared Error: {mse}\n",
    "        ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9232420-914f-4d62-8850-3b42de72d365",
   "metadata": {},
   "source": [
    "### API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b4c65e9-f845-44da-844c-4d021da6c93f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'subbreddit': 'lifehacks',\n",
    "    'size': 500,\n",
    "    'sort': 'desc',\n",
    "    'sort_type': 'created_utc',\n",
    "    'metadata': True,\n",
    "    'is_video': False,\n",
    "    'after': '',\n",
    "    'before': ''\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3b65f83-5634-4e9c-b1d6-ac8573bbc8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_call(subreddit, size=25, before=''):\n",
    "    url = f\"https://api.pushshift.io/reddit/search/submission/?subreddit={subreddit}&size={size}&metadata=True&is_video=False&before={before}\"\n",
    "    req = requests.get(url)\n",
    "    \n",
    "    if req.status_code != 200:\n",
    "        return \"Error: API call failed.\"\n",
    "    else:\n",
    "        call = req.json()\n",
    "        return call['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9b3065-2b6c-4aa6-adeb-ecfe0efee033",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e2e1f34-019e-4201-8d29-a87577ab410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_wrangling(dict_, keys, api_call):\n",
    "    error_log = [] #used to capture indices with missing data\n",
    "    for i in range(len(api_call)):\n",
    "        for key in keys:\n",
    "            try:\n",
    "                dict_[key].append(api_call[i][key])\n",
    "            except:\n",
    "                error_log.append(f\"Error on index: {i}\\nkey \\\"{key}\\\" not found.\")\n",
    "                dict_[key].append(None) #if there is not data, set it to null\n",
    "    return {'data': dict_, 'error_log': error_log}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25204946-4b90-4fee-b2fe-8e4c6407d90f",
   "metadata": {},
   "source": [
    "## Model Instantiation and Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64fcb7d8-846b-4e67-b906-7f96663c05ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(estimator, X_train, X_test, y_train, y_test):\n",
    "    estimator.fit(X_train, y_train)\n",
    "    preds = estimator.predict(X_test)\n",
    "    \n",
    "    print(f'''\n",
    "        Training Accuracy Score: {estimator.score(X_train, y_train)}\n",
    "        Test Accuracy Score: {estimator.score(X_train, y_train)}\n",
    "        \n",
    "        --- Performance on unseen data ----\n",
    "        Balance Accuracy: {balanced_accuracy_score(y_test, preds)}\n",
    "        Recall (Sensitivity): {recall_score(y_test, preds)}\n",
    "        Specificity: {recall_score(y_test, preds, pos_label=0)}\n",
    "        Precision: {precision_score(y_test, preds)}\n",
    "        F1 Score: {f1_score(y_test, preds)}\n",
    "        ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e86a852-1434-4971-8507-956e37ef083a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
