{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c726bc7-bd4d-48cd-a1cd-0436ab785641",
   "metadata": {},
   "source": [
    "# Production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653c621e-ded0-4378-a21f-4f3e1d46c7fc",
   "metadata": {},
   "source": [
    "In this notebook we analyze the results of our best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da318927-fa6d-4f09-8351-4b7d5a912a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "## metrics\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, recall_score, precision_score, f1_score, confusion_matrix, \n",
    "                            ConfusionMatrixDisplay, plot_roc_curve, roc_auc_score, classification_report, RocCurveDisplay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9faac8f-911a-40e8-9c33-62fcdc84f568",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3cd1bf-d82e-4524-9903-bfca57193d91",
   "metadata": {},
   "source": [
    "Let's create a baseline model to compare our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7619941a-656d-46dc-87de-5224f88dc672",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = DummyClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "3cfa2741-8463-4ad0-8146-16883b6cdf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Training Accuracy Score: 0.7587506731287023\n",
      "        Test Accuracy Score: 0.7592891760904685\n",
      "        \n",
      "        --- Performance on unseen data ----\n",
      "        Recall (Sensitivity): 0.0\n",
      "        Specificity: 1.0\n",
      "        Precision: 0.0\n",
      "        \n",
      "        Balance Accuracy: 0.5\n",
      "        F1 Score: 0.0\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings(record=True): #ignore warnings\n",
    "    make_model(dc, X_train_cvec, X_test_cvec, y_train_encoded, y_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a6f43-eac0-4978-8d32-065882989114",
   "metadata": {},
   "source": [
    "- Regarding accuracy, 75.9% of our predictions correctly predicted if a posts belongs to `r/LifeProTips` or `r/lifehacks`.\n",
    "\n",
    "- Regarding recall, the proportion of correctly predicted `r/lifehacks` posts over actual `r/lifehacks` posts. <br />\n",
    "In this case we did not correctly predict any posts belong to `r/lifehacks`.\n",
    "\n",
    "- Regarding specificity, the proportion of correctly predicted `r/LifeProTips` posts over actual `r/LifeProTips` posts. <br />\n",
    "In this case we correctly predicted all `r/LifeProTips` posts that belong to `r/LifeProTips`.\n",
    "\n",
    "- Regarding precision, the proportion of correct `r/lifehacks` predictions over all `r/lifehacks` predictions. <br />\n",
    "Since we did not predict any post to belong to `r/lifehacks`, our precision score cannot be calculated (can't divide by zero). Defaults to zero instead.\n",
    "\n",
    "- Our balanced accuracy score is the average of our recall and specificty score. It's important for us here because our data is imbalanced (far more cases of `r/LifeProTips` than `r/lifehacks`). <br />\n",
    "Here we can see that our model is not doing a good job of predicting both classes correctly.\n",
    "\n",
    "- Our F1 score is perhaps the most important metric here, because it measures the effects of recall and precision, the two most relevant metrics in our predictive model. </ br>\n",
    "Since our recall was 0, our F1 score is 0 as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636d88f6-75da-4cc3-91c3-ff1913bad59a",
   "metadata": {},
   "source": [
    "Given that our recall score is `0`, and our specifity score is `1`, our baseline model simply predicted all posts in the dataset to belong to `r/LifeProTips`! <br />\n",
    "That means we incorrectly predicted about `30%` of our data, hence our accuracy score of 0.759."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
